{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Micrograd from Scratch\n",
    "\n",
    "**Video**: [Karpathy's Neural Networks: Zero to Hero](https://www.youtube.com/watch?v=VMj-3S1tku0)\n",
    "\n",
    "---\n",
    "\n",
    "## How This Notebook Works\n",
    "\n",
    "1. **Part 0**: See the magic first (finished product doing something cool)\n",
    "2. **Parts 1-6**: Build it yourself, piece by piece, with heavy annotations\n",
    "3. **Part 7-8**: Use it to train a real neural network\n",
    "\n",
    "Let's start by seeing what we're building toward!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext claude_code_jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 0: See the Magic First\n",
    "\n",
    "Before we build anything, let's see what micrograd can do.\n",
    "\n",
    "Here's the finished Value class (don't worry about understanding it yet):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE FINISHED PRODUCT - just run this, we'll understand it later\n",
    "import math\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data:.4f}, grad={self.grad:.4f})\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other): return self + other\n",
    "    def __rmul__(self, other): return self * other\n",
    "    def __neg__(self): return self * -1\n",
    "    def __sub__(self, other): return self + (-other)\n",
    "    \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "\n",
    "print(\"Value class loaded! Let's see what it can do...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Magic in Action\n",
    "\n",
    "Remember in the calculus notebook, you manually computed gradients for `L = (a √ó b) + c`?\n",
    "\n",
    "**Watch micrograd do it automatically:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some values\n",
    "a = Value(2.0)\n",
    "b = Value(3.0)\n",
    "c = Value(4.0)\n",
    "\n",
    "# Do math (forward pass)\n",
    "L = (a * b) + c\n",
    "\n",
    "print(f\"L = (a √ó b) + c = ({a.data} √ó {b.data}) + {c.data} = {L.data}\")\n",
    "print()\n",
    "print(\"Now watch this...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE LINE computes ALL gradients!\n",
    "L.backward()\n",
    "\n",
    "print(\"After L.backward():\")\n",
    "print(f\"  a.grad = {a.grad}  ‚Üê 'nudge a up by 1, L goes up by {a.grad}'\")\n",
    "print(f\"  b.grad = {b.grad}  ‚Üê 'nudge b up by 1, L goes up by {b.grad}'\")\n",
    "print(f\"  c.grad = {c.grad}  ‚Üê 'nudge c up by 1, L goes up by {c.grad}'\")\n",
    "print()\n",
    "print(\"Remember from calculus notebook:\")\n",
    "print(\"  - Multiplication rule: gradient = the OTHER input\")\n",
    "print(f\"  - So a.grad = b = {b.data} ‚úì\")\n",
    "print(f\"  - And b.grad = a = {a.data} ‚úì\")\n",
    "print(\"  - Addition rule: gradient flows through equally\")\n",
    "print(f\"  - So c.grad = 1 ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§Ø That's It!\n",
    "\n",
    "With plain Python numbers, `2 * 3 + 4 = 10` and that's all you get.\n",
    "\n",
    "With Value objects, you get the answer **AND** you can ask \"how does each input affect the output?\"\n",
    "\n",
    "**This is the entire foundation of deep learning.** A neural network is just a big expression like this, and `.backward()` tells us how to adjust every weight.\n",
    "\n",
    "---\n",
    "\n",
    "Now let's build this ourselves, step by step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: The Dumb Way - Numerical Derivatives\n",
    "\n",
    "Before we automate anything, let's remember how to compute derivatives \"by hand\" (with code).\n",
    "\n",
    "**The method:** Nudge the input, see how much the output changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# A simple function\n",
    "def f(x):\n",
    "    return 3 * x**2 - 4 * x + 5\n",
    "\n",
    "# Plot it\n",
    "xs = np.arange(-5, 5, 0.25)\n",
    "ys = f(xs)\n",
    "plt.plot(xs, ys)\n",
    "plt.title('f(x) = 3x¬≤ - 4x + 5')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the derivative at x = 3 using the \"nudge\" method\n",
    "x = 3.0\n",
    "h = 0.0001  # tiny nudge\n",
    "\n",
    "# How much does f change when we nudge x?\n",
    "derivative = (f(x + h) - f(x)) / h\n",
    "\n",
    "print(f\"At x = {x}:\")\n",
    "print(f\"  f(x) = {f(x)}\")\n",
    "print(f\"  f(x + h) = {f(x + h)}\")\n",
    "print(f\"  Derivative ‚âà {derivative:.4f}\")\n",
    "print()\n",
    "print(f\"Check: derivative of 3x¬≤ - 4x + 5 is 6x - 4\")\n",
    "print(f\"       At x=3: 6(3) - 4 = {6*3 - 4} ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why h = 0.0001?\n",
    "\n",
    "- **Too big** (h = 1): You get the average slope over a big range, not the instant slope\n",
    "- **Too small** (h = 0.0000000001): Computer floating point errors mess things up\n",
    "- **Just right** (h ‚âà 0.0001): Small enough to be accurate, big enough to avoid rounding errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Multiple Inputs - The Problem\n",
    "\n",
    "What if we have multiple variables? Let's compute `d = a * b + c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our expression: d = a * b + c\n",
    "a = 2.0\n",
    "b = -3.0\n",
    "c = 10.0\n",
    "\n",
    "d = a * b + c\n",
    "print(f\"d = a * b + c = {a} * {b} + {c} = {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find dd/da, dd/db, dd/dc, we have to nudge EACH variable separately\n",
    "h = 0.0001\n",
    "\n",
    "def compute_d(a, b, c):\n",
    "    return a * b + c\n",
    "\n",
    "d_original = compute_d(a, b, c)\n",
    "\n",
    "# Nudge a\n",
    "dd_da = (compute_d(a + h, b, c) - d_original) / h\n",
    "print(f\"dd/da = {dd_da:.4f}  (expected: b = {b})\")\n",
    "\n",
    "# Nudge b\n",
    "dd_db = (compute_d(a, b + h, c) - d_original) / h\n",
    "print(f\"dd/db = {dd_db:.4f}  (expected: a = {a})\")\n",
    "\n",
    "# Nudge c\n",
    "dd_dc = (compute_d(a, b, c + h) - d_original) / h\n",
    "print(f\"dd/dc = {dd_dc:.4f}  (expected: 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem: This is Slow!\n",
    "\n",
    "For 3 variables, we needed 3 separate nudge computations.\n",
    "\n",
    "For a neural network with **1 million** weights, we'd need **1 million** nudge computations.\n",
    "\n",
    "**Micrograd's solution:** Compute ALL gradients in ONE backward pass using the chain rule.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Building the Value Class\n",
    "\n",
    "Now we build the magic box that remembers its history.\n",
    "\n",
    "### What Value Needs to Track\n",
    "\n",
    "```python\n",
    "self.data      # The actual number (like 2.0)\n",
    "self.grad      # The gradient (dL/d_this_value) - starts at 0\n",
    "self._prev     # Who made me? (the parent Values)\n",
    "self._backward # How to compute parent gradients (the calculus rule)\n",
    "self._op       # What operation made me? (for visualization)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1: Just the container (no operations yet)\n",
    "\n",
    "class Value:\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data      # The number\n",
    "        self.grad = 0.0       # Gradient (computed later)\n",
    "        self._prev = set()    # Parents (empty for now)\n",
    "        self._backward = lambda: None  # Backward function (does nothing yet)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "# Test it\n",
    "a = Value(2.0)\n",
    "print(a)\n",
    "print(f\"a.data = {a.data}\")\n",
    "print(f\"a.grad = {a.grad}\")\n",
    "print(f\"a._prev = {a._prev}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Addition\n",
    "\n",
    "When we do `c = a + b`, we want:\n",
    "1. `c.data` = sum of the numbers\n",
    "2. `c._prev` = `{a, b}` (c remembers its parents)\n",
    "3. `c._backward` = function that applies the **addition rule**\n",
    "\n",
    "**Remember from calculus:** For `z = x + y`, both inputs get the gradient equally.\n",
    "- `dx = dz √ó 1`\n",
    "- `dy = dz √ó 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2: Add addition\n",
    "\n",
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=()):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_children)  # Now we can pass in parents!\n",
    "        self._backward = lambda: None\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        # Create the output Value\n",
    "        out = Value(\n",
    "            self.data + other.data,  # The sum\n",
    "            (self, other)            # Remember parents!\n",
    "        )\n",
    "        \n",
    "        # Define how gradients flow backward through addition\n",
    "        # Addition rule: gradient flows through equally (√ó 1)\n",
    "        def _backward():\n",
    "            self.grad += out.grad   # my gradient += output's gradient √ó 1\n",
    "            other.grad += out.grad  # other's gradient += output's gradient √ó 1\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "# Test it\n",
    "a = Value(2.0)\n",
    "b = Value(3.0)\n",
    "c = a + b\n",
    "\n",
    "print(f\"a = {a}\")\n",
    "print(f\"b = {b}\")\n",
    "print(f\"c = a + b = {c}\")\n",
    "print(f\"c._prev = {c._prev}  ‚Üê c remembers its parents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Manually Test the Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(2.0)\n",
    "b = Value(3.0)\n",
    "c = a + b  # c = 5\n",
    "\n",
    "# Pretend c is our final output, so dc/dc = 1\n",
    "c.grad = 1.0\n",
    "\n",
    "# Now call backward to flow gradients to parents\n",
    "c._backward()\n",
    "\n",
    "print(f\"c.grad = {c.grad} (we set this to 1)\")\n",
    "print(f\"a.grad = {a.grad} (should be 1 - addition passes gradient through)\")\n",
    "print(f\"b.grad = {b.grad} (should be 1 - addition passes gradient through)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Multiplication\n",
    "\n",
    "**Remember from calculus:** For `z = x √ó y`, each input's gradient is the OTHER input.\n",
    "- `dx = dz √ó y`\n",
    "- `dy = dz √ó x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 3: Add multiplication\n",
    "\n",
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=()):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_children)\n",
    "        self._backward = lambda: None\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other))\n",
    "        def _backward():\n",
    "            # Addition rule: gradient flows through equally\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other))\n",
    "        def _backward():\n",
    "            # Multiplication rule: gradient = other input √ó output gradient\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "# Test: d = a * b + c\n",
    "a = Value(2.0)\n",
    "b = Value(3.0)\n",
    "c = Value(4.0)\n",
    "\n",
    "d = a * b  # intermediate: 6\n",
    "L = d + c  # final: 10\n",
    "\n",
    "print(f\"a = {a.data}, b = {b.data}, c = {c.data}\")\n",
    "print(f\"d = a * b = {d.data}\")\n",
    "print(f\"L = d + c = {L.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually do backward pass\n",
    "# Step 1: Start at L, set its gradient to 1\n",
    "L.grad = 1.0\n",
    "\n",
    "# Step 2: L = d + c (addition) ‚Üí both get gradient 1\n",
    "L._backward()\n",
    "print(f\"After L._backward():\")\n",
    "print(f\"  d.grad = {d.grad} (addition: passes through)\")\n",
    "print(f\"  c.grad = {c.grad} (addition: passes through)\")\n",
    "\n",
    "# Step 3: d = a * b (multiplication) ‚Üí gradient = other input\n",
    "d._backward()\n",
    "print(f\"After d._backward():\")\n",
    "print(f\"  a.grad = {a.grad} (multiplication: = b = {b.data})\")\n",
    "print(f\"  b.grad = {b.grad} (multiplication: = a = {a.data})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Quick Check\n",
    "\n",
    "These are the same gradients we computed by hand in the calculus notebook!\n",
    "\n",
    "- `a.grad = 3` because a is multiplied by b=3\n",
    "- `b.grad = 2` because b is multiplied by a=2\n",
    "- `c.grad = 1` because c is just added\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Automating the Backward Pass\n",
    "\n",
    "We don't want to call `_backward()` on each node manually. We want ONE call: `L.backward()`\n",
    "\n",
    "### The Challenge: Order Matters!\n",
    "\n",
    "We must process nodes in the right order:\n",
    "- `L` first (set its grad to 1)\n",
    "- Then `d` and `c` (they depend on L's grad)\n",
    "- Then `a` and `b` (they depend on d's grad)\n",
    "\n",
    "This is called **topological sort** - process parents before children (when going backward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 4: Add automatic backward()\n",
    "\n",
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=()):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_children)\n",
    "        self._backward = lambda: None\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other))\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other))\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        # Step 1: Build a list of all nodes in topological order\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        \n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:  # Visit all parents first\n",
    "                    build_topo(child)\n",
    "                topo.append(v)  # Then add this node\n",
    "        \n",
    "        build_topo(self)\n",
    "        \n",
    "        # Step 2: Start with gradient of 1 at the output\n",
    "        self.grad = 1.0\n",
    "        \n",
    "        # Step 3: Go backward through the list, calling each _backward()\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "\n",
    "print(\"Value class with backward() ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it! Same expression: L = a * b + c\n",
    "a = Value(2.0)\n",
    "b = Value(3.0)\n",
    "c = Value(4.0)\n",
    "\n",
    "L = a * b + c\n",
    "\n",
    "print(f\"Before backward():\")\n",
    "print(f\"  L = {L.data}\")\n",
    "print(f\"  a.grad = {a.grad}, b.grad = {b.grad}, c.grad = {c.grad}\")\n",
    "print()\n",
    "\n",
    "# ONE CALL computes all gradients!\n",
    "L.backward()\n",
    "\n",
    "print(f\"After L.backward():\")\n",
    "print(f\"  a.grad = {a.grad} (should be b = 3)\")\n",
    "print(f\"  b.grad = {b.grad} (should be a = 2)\")\n",
    "print(f\"  c.grad = {c.grad} (should be 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéâ It Works!\n",
    "\n",
    "One call to `L.backward()` computed all three gradients automatically.\n",
    "\n",
    "This is the core of backpropagation!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Making it Robust\n",
    "\n",
    "Our Value class works, but it can't handle:\n",
    "- `2 * a` (number on the left)\n",
    "- `a - b` (subtraction)\n",
    "- `a ** 2` (powers)\n",
    "\n",
    "Let's add these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 5: More operations\n",
    "\n",
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_children)\n",
    "        self._backward = lambda: None\n",
    "        self._op = _op  # For debugging/visualization\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data:.4f}, grad={self.grad:.4f})\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        # Handle: Value + number\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):  # number + Value\n",
    "        return self + other\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):  # number * Value\n",
    "        return self * other\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        # Only supporting int/float powers for simplicity\n",
    "        assert isinstance(other, (int, float))\n",
    "        out = Value(self.data ** other, (self,), f'**{other}')\n",
    "        def _backward():\n",
    "            # Power rule: d/dx(x^n) = n * x^(n-1)\n",
    "            self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __neg__(self):  # -Value\n",
    "        return self * -1\n",
    "    \n",
    "    def __sub__(self, other):  # Value - other\n",
    "        return self + (-other)\n",
    "    \n",
    "    def __rsub__(self, other):  # other - Value\n",
    "        return other + (-self)\n",
    "    \n",
    "    def __truediv__(self, other):  # Value / other\n",
    "        return self * other**-1\n",
    "    \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        \n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "\n",
    "print(\"Enhanced Value class ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all the new operations\n",
    "x = Value(3.0)\n",
    "\n",
    "# Test: f(x) = 2*x^2 - 5*x + 3\n",
    "y = 2 * x**2 - 5*x + 3\n",
    "\n",
    "print(f\"x = {x.data}\")\n",
    "print(f\"y = 2*x¬≤ - 5*x + 3 = {y.data}\")\n",
    "print()\n",
    "\n",
    "y.backward()\n",
    "print(f\"dy/dx = {x.grad}\")\n",
    "print(f\"Expected: 4*x - 5 = 4*{x.data} - 5 = {4*x.data - 5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Adding tanh (Activation Function)\n",
    "\n",
    "Neural networks need **activation functions** to be interesting. The simplest is `tanh`.\n",
    "\n",
    "```\n",
    "tanh(x) = (e^(2x) - 1) / (e^(2x) + 1)\n",
    "```\n",
    "\n",
    "Its derivative is: `d/dx tanh(x) = 1 - tanh(x)¬≤`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Version 6: Add tanh\n",
    "\n",
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_children)\n",
    "        self._backward = lambda: None\n",
    "        self._op = _op\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data:.4f}, grad={self.grad:.4f})\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other): return self + other\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other): return self * other\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float))\n",
    "        out = Value(self.data ** other, (self,), f'**{other}')\n",
    "        def _backward():\n",
    "            self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __neg__(self): return self * -1\n",
    "    def __sub__(self, other): return self + (-other)\n",
    "    def __rsub__(self, other): return other + (-self)\n",
    "    def __truediv__(self, other): return self * other**-1\n",
    "    \n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
    "        out = Value(t, (self,), 'tanh')\n",
    "        def _backward():\n",
    "            # Derivative of tanh: 1 - tanh¬≤\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "\n",
    "print(\"Value class with tanh ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a neuron-like computation!\n",
    "# neuron output = tanh(w1*x1 + w2*x2 + b)\n",
    "\n",
    "# inputs\n",
    "x1 = Value(2.0)\n",
    "x2 = Value(0.0)\n",
    "\n",
    "# weights\n",
    "w1 = Value(-3.0)\n",
    "w2 = Value(1.0)\n",
    "\n",
    "# bias\n",
    "b = Value(6.88137)\n",
    "\n",
    "# Forward pass: compute the neuron output\n",
    "n = x1*w1 + x2*w2 + b  # weighted sum\n",
    "o = n.tanh()            # activation\n",
    "\n",
    "print(f\"Neuron computation:\")\n",
    "print(f\"  x1={x1.data}, x2={x2.data}\")\n",
    "print(f\"  w1={w1.data}, w2={w2.data}\")\n",
    "print(f\"  b={b.data}\")\n",
    "print(f\"  n = x1*w1 + x2*w2 + b = {n.data:.4f}\")\n",
    "print(f\"  o = tanh(n) = {o.data:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "o.backward()\n",
    "\n",
    "print(f\"Gradients (how each input affects the output):\")\n",
    "print(f\"  do/dx1 = {x1.grad:.4f}\")\n",
    "print(f\"  do/dx2 = {x2.grad:.4f}\")\n",
    "print(f\"  do/dw1 = {w1.grad:.4f}  ‚Üê This tells us how to adjust w1!\")\n",
    "print(f\"  do/dw2 = {w2.grad:.4f}  ‚Üê This tells us how to adjust w2!\")\n",
    "print(f\"  do/db  = {b.grad:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 7: Building a Neural Network\n",
    "\n",
    "Now we have all the pieces. Let's build actual neural network components!\n",
    "\n",
    "### What We'll Build\n",
    "\n",
    "```\n",
    "Neuron: takes inputs, multiplies by weights, adds bias, applies tanh\n",
    "Layer:  a bunch of neurons\n",
    "MLP:    Multiple Layers stacked (Multi-Layer Perceptron)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Neuron:\n",
    "    \"\"\"A single neuron: weighted sum of inputs + bias, through tanh\"\"\"\n",
    "    \n",
    "    def __init__(self, nin):  # nin = number of inputs\n",
    "        # Random weights between -1 and 1\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1, 1))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # w ¬∑ x + b (dot product + bias)\n",
    "        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        return act.tanh()\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "# Test a neuron\n",
    "n = Neuron(3)  # Neuron with 3 inputs\n",
    "x = [1.0, 2.0, 3.0]\n",
    "out = n(x)\n",
    "\n",
    "print(f\"Neuron with 3 inputs\")\n",
    "print(f\"  Weights: {[f'{w.data:.3f}' for w in n.w]}\")\n",
    "print(f\"  Bias: {n.b.data:.3f}\")\n",
    "print(f\"  Input: {x}\")\n",
    "print(f\"  Output: {out.data:.4f}\")\n",
    "print(f\"  Number of parameters: {len(n.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"A layer of neurons\"\"\"\n",
    "    \n",
    "    def __init__(self, nin, nout):  # nin inputs, nout neurons\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "# Test a layer\n",
    "layer = Layer(3, 4)  # 3 inputs, 4 neurons\n",
    "x = [1.0, 2.0, 3.0]\n",
    "out = layer(x)\n",
    "\n",
    "print(f\"Layer: 3 inputs ‚Üí 4 neurons\")\n",
    "print(f\"  Outputs: {[f'{o.data:.3f}' for o in out]}\")\n",
    "print(f\"  Number of parameters: {len(layer.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"Multi-Layer Perceptron: stack of layers\"\"\"\n",
    "    \n",
    "    def __init__(self, nin, nouts):  # nouts is list of layer sizes\n",
    "        sz = [nin] + nouts  # e.g., [3, 4, 4, 1] for 3‚Üí4‚Üí4‚Üí1\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "# Create a network: 3 inputs ‚Üí 4 neurons ‚Üí 4 neurons ‚Üí 1 output\n",
    "net = MLP(3, [4, 4, 1])\n",
    "\n",
    "x = [2.0, 3.0, -1.0]\n",
    "out = net(x)\n",
    "\n",
    "print(f\"MLP: 3 ‚Üí 4 ‚Üí 4 ‚Üí 1\")\n",
    "print(f\"  Input: {x}\")\n",
    "print(f\"  Output: {out.data:.4f}\")\n",
    "print(f\"  Total parameters: {len(net.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture\n",
    "\n",
    "```\n",
    "Input (3)     Hidden (4)    Hidden (4)    Output (1)\n",
    "   ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè\n",
    "   ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè              \n",
    "   ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè\n",
    "              ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè\n",
    "              \n",
    "41 parameters = (3√ó4 + 4) + (4√ó4 + 4) + (4√ó1 + 1)\n",
    "              = 16 + 20 + 5 = 41\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 8: Training!\n",
    "\n",
    "Now the grand finale: let's train this network to learn something!\n",
    "\n",
    "### The Training Loop\n",
    "\n",
    "```\n",
    "1. Forward pass: compute predictions\n",
    "2. Compute loss: how wrong are we?\n",
    "3. Backward pass: compute gradients\n",
    "4. Update: nudge parameters to reduce loss\n",
    "5. Repeat!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data: 4 examples\n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0]  # What we want the network to output\n",
    "\n",
    "print(\"Training data:\")\n",
    "for x, y in zip(xs, ys):\n",
    "    print(f\"  {x} ‚Üí {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fresh network\n",
    "net = MLP(3, [4, 4, 1])\n",
    "\n",
    "# Training loop!\n",
    "learning_rate = 0.1\n",
    "\n",
    "for step in range(100):\n",
    "    \n",
    "    # === FORWARD PASS ===\n",
    "    # Make predictions for all inputs\n",
    "    ypred = [net(x) for x in xs]\n",
    "    \n",
    "    # === COMPUTE LOSS ===\n",
    "    # Mean squared error: sum of (prediction - target)¬≤\n",
    "    loss = sum((yp - yt)**2 for yp, yt in zip(ypred, ys))\n",
    "    \n",
    "    # === BACKWARD PASS ===\n",
    "    # First, zero all gradients (important!)\n",
    "    for p in net.parameters():\n",
    "        p.grad = 0.0\n",
    "    # Then compute new gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # === UPDATE ===\n",
    "    # Nudge each parameter in the direction that reduces loss\n",
    "    for p in net.parameters():\n",
    "        p.data -= learning_rate * p.grad\n",
    "    \n",
    "    # Print progress\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step:3d}: loss = {loss.data:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal loss: {loss.data:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check final predictions!\n",
    "print(\"Final predictions vs targets:\")\n",
    "print()\n",
    "for x, y in zip(xs, ys):\n",
    "    pred = net(x)\n",
    "    print(f\"  Input: {x}\")\n",
    "    print(f\"  Predicted: {pred.data:+.4f}  Target: {y:+.1f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üéâ Congratulations!\n",
    "\n",
    "You just built:\n",
    "\n",
    "1. **An autograd engine** (Value class) - ~50 lines\n",
    "2. **A neural network library** (Neuron, Layer, MLP) - ~30 lines  \n",
    "3. **A training loop** - ~15 lines\n",
    "\n",
    "This is the **entire foundation** of modern deep learning!\n",
    "\n",
    "PyTorch, TensorFlow, JAX - they're all doing exactly this, just:\n",
    "- More operations (convolution, attention, etc.)\n",
    "- GPU acceleration\n",
    "- More optimizers (Adam, etc.)\n",
    "- Better numerical stability\n",
    "\n",
    "But the core idea is **exactly what you just built**.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Concept | What It Means |\n",
    "|---------|---------------|\n",
    "| `.data` | The actual number |\n",
    "| `.grad` | How much the loss changes if we nudge this value |\n",
    "| `._prev` | Who made me (parents in the computation graph) |\n",
    "| `._backward` | The calculus rule for this operation |\n",
    "| `.backward()` | Walk the graph backwards, fill in all gradients |\n",
    "| Training | Forward ‚Üí Loss ‚Üí Backward ‚Üí Update ‚Üí Repeat |\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** makemore (building a character-level language model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
